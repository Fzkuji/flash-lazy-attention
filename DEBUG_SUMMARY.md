# Multi-Head Gradient Bug - Debug Summary

## Problem Statement
**Only Head 0 receives gradients for `bias` and `tau` parameters in the Triton backward kernel.**

### Observed Behavior
```
Head 0: bias_grad=âœ…, tau_grad=âœ… âœ…
Head 1: bias_grad=âŒ, tau_grad=âŒ âŒ
Head 2: bias_grad=âŒ, tau_grad=âŒ âŒ
Head 3: bias_grad=âŒ, tau_grad=âŒ âŒ
```

### Expected Behavior
All heads should receive non-zero gradients for both `bias` and `tau`.

## Test Results

### âœ… Test 1: Basic atomic_add (PASSED)
- **File**: `test_atomic_add.py`
- **Result**: All heads successfully write to output using `tl.atomic_add`
- **Conclusion**: Basic atomic_add functionality works correctly

### âœ… Test 2: 2D atomic_add with strides (PASSED)
- **File**: `test_atomic_add_detailed.py`
- **Result**: Both `dBias` (2D) and `dTau` (1D) atomic operations work for all heads
- **Key findings**:
  - 2D indexing: `DBias + h_idx * stride_bh + dist_clamped * stride_bw` âœ…
  - 1D indexing: `DTau + h_idx` âœ…
- **Conclusion**: Atomic operations with head-based indexing work correctly

### â³ Test 3: Minimal backward kernel (READY TO RUN)
- **File**: `test_minimal_backward.py`
- **Status**: All syntax errors fixed, ready for testing
- **Purpose**: Test simplified backward logic with grid parallelization (H, L)
- **What it tests**:
  - Each (h_idx, l_idx) thread adds gradients via atomic_add
  - Grid: (H=4, L=8) - 32 total threads
  - Expected: All 4 heads should have non-zero dbias and dtau
- **Run with**: `git pull && python test_minimal_backward.py`

## Key Code Locations

### Backward Kernel (lazy_attention_triton.py)
**Main gradient computation** (lines 261-359):
```python
@triton.jit
def _lazy_bwd_kernel_dq(...):
    h_idx = tl.program_id(1)  # Head index from grid

    # dBias gradient (line 351)
    tl.atomic_add(DBias + h_idx * stride_bh + dist_clamped * stride_bw, dbias_val, mask=valid_mask)

    # dTau gradient (line 358)
    tl.atomic_add(DTau + h_idx, tl.sum(dtau_acc))
```

**Kernel Launch** (line 556):
```python
grid_m = (triton.cdiv(L, BLOCK_M), H, B)  # Grid includes H dimension
_lazy_bwd_kernel_dq[grid_m](...)
```

### Preprocess Kernel (lines 174-258)
Computes `delta` for use in main backward pass. Uses same grid configuration.

## Hypotheses

### âŒ Ruled Out
1. **Atomic add bug**: Tests 1 & 2 prove atomic_add works
2. **Stride calculation error**: Test 2 shows stride-based indexing works
3. **Grid configuration**: Grid `(triton.cdiv(L, BLOCK_M), H, B)` is correct
4. **Float32 precision**: Not the root cause (as user noted)

### ğŸ” Still Investigating
1. **mask_relu zeroing gradients**: Line 342-355
   - `mask_relu = p_term > 0` might be True only for h_idx=0
   - This would zero out `dbias_val` and `term_tau` for other heads

2. **LSE/Delta values**: Lines 299-300, 338-345
   - If `lse` or `delta` are incorrect for h_idx>0, gradients would be wrong
   - Check if preprocess kernel computes delta correctly for all heads

3. **Causal mask logic**: Lines 322-332
   - `dist = offs_m[:, None] - offs_n[None, :]`
   - `in_window = (dist >= 0) & (dist < window_size)`
   - Might have head-dependent issues

4. **Data dependencies**:
   - Check if `q`, `k`, `v`, `bias`, `tau` are loaded correctly for all heads
   - Verify forward pass produces valid outputs for all heads

## Next Steps

1. âœ… Run `test_minimal_backward.py` to verify simplified logic
2. Add debug prints inside actual backward kernel to check:
   - `h_idx` values being executed
   - `mask_relu` distribution across heads
   - `dbias_val` and `dtau_acc` values before atomic_add
3. Compare preprocess kernel delta values across heads
4. Verify forward kernel produces correct outputs for all heads

## Files Modified
- `c:\Users\fzkuj\Projects\adasplash\lazy_attention_triton.py` - Main kernel file
- `test_head_gradients.py` - Primary test showing the bug
- `test_atomic_add.py` - Basic atomic test (PASSED)
- `test_atomic_add_detailed.py` - Stride-based atomic test (PASSED)
- `test_minimal_backward.py` - Simplified backward test (PENDING)

## Environment
- Triton version: (check with `pip show triton`)
- PyTorch version: (check with `pip show torch`)
- CUDA version: (check with `nvidia-smi`)
- Test configuration: B=1, H=4, L=8, D=16, window_size=16
